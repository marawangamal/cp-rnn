runs: 3
train:
  epochs: 25
  lr: 0.0001
  batch_size: 128
  seq_len: 50
  grad_clip: inf
  hpopt: False
  verbose: False
model:
  name: mirnn # lstmpt, cprnn, 2rnn
  input_size: 0 # 0 means one-hot encoding else give an integer embedding size
  hidden_size: 2048
  rank: 64
  dropout: 0
  gate: tanh # tanh, sigmoid, identity
data:
  path: data/processed/ptb # Path to the data
  tokenizer: char # char, word
  output: runs
eval:
  compute_grads: False

# Options for data paths:
#   data/processed/anna
#   data/processed/ptb

# Experiments
# e25_l0.0001_b1024_s50_ginf_nmirnn_i200_h2048_r64_d0
# e25_l0.0001_b1024_s50_ginf_ncprnn_i200_h2048_r64_d0

# Word Level
#runs: 1
#train:
#  epochs: 100
#  lr: 20
#  batch_size: 128
#  seq_len: 50
#  grad_clip: inf
#  hpopt: False
#  verbose: False
#model:
#  name: lstm # lstmpt, cprnn, 2rnn
#  input_size: 256 # 0 means one-hot encoding else give an integer embedding size
#  hidden_size: 1024
#  rank: 64
#  dropout: 0.2
#  gate: tanh # tanh, sigmoid, identity
#data:
#  path: data/processed/ptb # Path to the data
#  tokenizer: word # char, word
#  output: runs
#eval:
#  compute_grads: False